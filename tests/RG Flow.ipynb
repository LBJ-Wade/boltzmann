{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from boltzmann.data.mnist import load_mnist\n",
    "from boltzmann.restricted.base import train\n",
    "from boltzmann.restricted.bernoulli.common import HintonInitializer\n",
    "from boltzmann.restricted.bernoulli.dense import (\n",
    "    DenseBernoulliRBM, initialize_fantasy_latent, enlarge_latent,\n",
    "    get_reconstruction_error)\n",
    "from boltzmann.utils import History, ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "IMAGE_SIZE = (16, 16)\n",
    "LATENT_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "SEED = 42\n",
    "\n",
    "INCREMENT = 8\n",
    "\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "(X, y), _ = load_mnist(image_size=IMAGE_SIZE, binarize=True, minval=0, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(epochs: int, cache_path: str = None):\n",
    "    ambient_size = IMAGE_SIZE[0] * IMAGE_SIZE[1]\n",
    "    rbm = DenseBernoulliRBM(ambient_size, LATENT_SIZE, HintonInitializer(X, seed=SEED))\n",
    "    if cache_path is None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "        epochs = 20\n",
    "        # epochs = 1  # XXX: test!\n",
    "        dataset = dataset.shuffle(10000, seed=SEED).repeat(epochs).batch(BATCH_SIZE)\n",
    "        fantasy_latent = initialize_fantasy_latent(rbm.latent_size, BATCH_SIZE, seed=SEED)\n",
    "        optimizer = tf.optimizers.Adam()\n",
    "        fantasy_latent = train(rbm, optimizer, dataset, fantasy_latent)\n",
    "    else:\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                U, bv, bh, fantasy_latent = pickle.load(f)\n",
    "            rbm.kernel.assign(U)\n",
    "            rbm.ambient_bias.assign(bv)\n",
    "            rbm.latent_bias.assign(bh)\n",
    "            fantasy_latent = tf.constant(fantasy_latent)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f'[WARNING]: Cannot find file \"{cache_path}\", create new file on that path.')\n",
    "            rbm, fantasy_latent = build_and_train(epochs, cache_path=None)\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            U = rbm.kernel.numpy()\n",
    "            bv = rbm.ambient_bias.numpy()\n",
    "            bh = rbm.latent_bias.numpy()\n",
    "            pickle.dump((U, bv, bh, fantasy_latent.numpy()), f)\n",
    "    return rbm, fantasy_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rbm, base_fantasy_latent = build_and_train(1, cache_path='../dat/base_rbm_for_rg_flow.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reconstruction_error(base_rbm, X[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "history = History()\n",
    "rbm = deepcopy(base_rbm)\n",
    "fantasy_latent = deepcopy(base_fantasy_latent)\n",
    "iter_step = 0\n",
    "\n",
    "def log(iter_step):\n",
    "    history.log(iter_step, 'kernel', rbm.kernel.numpy())\n",
    "    history.log(iter_step, 'ambient_bias', rbm.ambient_bias.numpy())\n",
    "    history.log(iter_step, 'latent_bias', rbm.latent_bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinite loop of incrementing\n",
    "while True:\n",
    "    print('\\n')\n",
    "    print(f'The {iter_step + 1}th interation......')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "    epochs = 10  # enough epochs for ensuring the convergence of training.\n",
    "    # epochs = 1  # XXX: test!\n",
    "    dataset = dataset.shuffle(10000, seed=SEED).repeat(epochs).batch(BATCH_SIZE)\n",
    "    inc_rbm, inc_fantasy_latent = enlarge_latent(rbm, fantasy_latent, INCREMENT)\n",
    "    optimizer = tf.optimizers.Adam()\n",
    "    inc_fantasy_latent = train(inc_rbm, optimizer, dataset, inc_fantasy_latent)\n",
    "\n",
    "    rbm, fantasy_latent, iter_step = inc_rbm, inc_fantasy_latent, iter_step + 1\n",
    "\n",
    "    log(iter_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current latent size:', rbm.latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = sorted(list(history.logs.keys()))\n",
    "kernel_diff_hist = []\n",
    "for i, j in zip(steps[:-1], steps[1:]):\n",
    "    U_i = history.logs[i]['kernel'][:, :LATENT_SIZE]\n",
    "    U_j = history.logs[j]['kernel'][:, :LATENT_SIZE]\n",
    "    kernel_diff_hist.append(U_j - U_i)\n",
    "kernel_diff_hist = np.stack(kernel_diff_hist, axis=0)\n",
    "kernel_diff_hist = ExponentialMovingAverage(0.9)(kernel_diff_hist, axis=0).numpy()\n",
    "\n",
    "plt.plot(steps[1:], np.zeros_like(steps[1:]), '--', label='zero')\n",
    "\n",
    "def plot_confidence_region(confidence, **plot_kwargs):\n",
    "    lower = [np.quantile(x.reshape([-1]), (1 - confidence) / 2) for x in kernel_diff_hist]\n",
    "    upper = [np.quantile(x.reshape([-1]), 1 - (1 - confidence) / 2) for x in kernel_diff_hist]\n",
    "    plt.fill_between(steps[1:], lower, upper,\n",
    "                     label=f'{(confidence * 100):.2f}% confidence region',\n",
    "                     **plot_kwargs)\n",
    "\n",
    "plot_confidence_region(0.6827, alpha=0.5)\n",
    "plot_confidence_region(0.9544, alpha=0.25)\n",
    "plot_confidence_region(0.9973, alpha=0.25)\n",
    "\n",
    "plt.title('Averaged kernel difference history')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ]
}